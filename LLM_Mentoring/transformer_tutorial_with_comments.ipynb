{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸ¤– Transformer ëª¨ë¸ êµ¬í˜„ ì‹¤ìŠµ\n",
    "\n",
    "## í•™ë¶€ìƒì„ ìœ„í•œ ë‹¨ê³„ë³„ íŠœí† ë¦¬ì–¼\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **\"Attention is All You Need\"** ë…¼ë¬¸ì˜ Transformerë¥¼ \n",
    "ê°„ë‹¨í•œ ë²ˆì—­ íƒœìŠ¤í¬ë¥¼ í†µí•´ ì´í•´í•˜ê¸° ì‰½ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š í•™ìŠµ ëª©í‘œ\n",
    "1. Positional Encodingì˜ ì›ë¦¬ ì´í•´\n",
    "2. Multi-Head Attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„\n",
    "3. Encoder-Decoder êµ¬ì¡° ì´í•´\n",
    "4. ì „ì²´ Transformer ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## ğŸ”§ í™˜ê²½ ì„¤ì •\n",
    "\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  import í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ê²½ìš° PyTorch ì„¤ì¹˜ (Colabì—ëŠ” ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŒ)\n",
    "# !pip install torch\n",
    "\n",
    "# torch: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ (í…ì„œ ì—°ì‚°, ìë™ ë¯¸ë¶„, GPU ê°€ì†)\n",
    "import torch\n",
    "\n",
    "# torch.nn: ì‹ ê²½ë§ êµ¬ì„± ìš”ì†Œ (Layer, Loss function ë“±)\n",
    "import torch.nn as nn\n",
    "\n",
    "# torch.optim: ìµœì í™” ì•Œê³ ë¦¬ì¦˜ (Adam, SGD ë“±)\n",
    "import torch.optim as optim\n",
    "\n",
    "# math: ìˆ˜í•™ í•¨ìˆ˜ (sqrt, log ë“±)\n",
    "import math\n",
    "\n",
    "# numpy: ìˆ˜ì¹˜ ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch ë²„ì „ê³¼ CUDA(GPU) ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1"
   },
   "source": [
    "---\n",
    "\n",
    "## 1ï¸âƒ£ ë‹¨ê³„ 1: Positional Encoding (ìœ„ì¹˜ ì¸ì½”ë”©)\n",
    "\n",
    "TransformerëŠ” RNNê³¼ ë‹¬ë¦¬ **ìˆœì„œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤**.  \n",
    "ë”°ë¼ì„œ ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ê³µì‹\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "- `pos`: ìœ„ì¹˜ (0, 1, 2, ...)\n",
    "- `i`: ì°¨ì› ì¸ë±ìŠ¤\n",
    "- `d_model`: ì„ë² ë”© ì°¨ì›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "positional_encoding"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding í´ë˜ìŠ¤\n",
    "    \n",
    "    ì—­í• : ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì„ë² ë”©ì— ì¶”ê°€\n",
    "    ì´ìœ : TransformerëŠ” RNNê³¼ ë‹¬ë¦¬ ìˆœì„œ ì •ë³´ê°€ ì—†ì–´ì„œ ìœ„ì¹˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì•Œë ¤ì¤˜ì•¼ í•¨\n",
    "    \n",
    "    sin, cos í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ìœ„ì¹˜ë§ˆë‹¤ ê³ ìœ í•œ ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): ì„ë² ë”© ì°¨ì› (ì˜ˆ: 512)\n",
    "            max_len (int): ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ì˜ˆ: 5000)\n",
    "        \n",
    "        ì´ìœ : ë¯¸ë¦¬ max_len ê¸¸ì´ê¹Œì§€ì˜ ìœ„ì¹˜ ì¸ì½”ë”©ì„ ê³„ì‚°í•´ë‘ë©´\n",
    "              forward ì‹œ ë§¤ë²ˆ ê³„ì‚°í•  í•„ìš” ì—†ì–´ íš¨ìœ¨ì ì„\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # ìœ„ì¹˜ ì¸ì½”ë”© í–‰ë ¬ ìƒì„±: (max_len, d_model) í¬ê¸°ì˜ 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ í…ì„œ\n",
    "        # ì´ìœ : ê° ìœ„ì¹˜(í–‰)ë§ˆë‹¤ d_model ì°¨ì›ì˜ ìœ„ì¹˜ ë²¡í„°ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•¨\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # position: [0, 1, 2, ..., max_len-1]ì„ (max_len, 1) í˜•íƒœë¡œ ìƒì„±\n",
    "        # unsqueeze(1): (max_len,) -> (max_len, 1)ë¡œ ì°¨ì› ì¶”ê°€\n",
    "        # ì´ìœ : broadcastingì„ ìœ„í•´ ì—´ ë²¡í„° í˜•íƒœë¡œ ë§Œë“¦\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        # ì£¼íŒŒìˆ˜(frequency) ê³„ì‚°: ë…¼ë¬¸ì˜ 10000^(2i/d_model) ë¶€ë¶„\n",
    "        # torch.arange(0, d_model, 2): 0, 2, 4, ... (ì§ìˆ˜ ì¸ë±ìŠ¤)\n",
    "        # ì´ìœ : sinê³¼ cosë¥¼ ì§ìˆ˜/í™€ìˆ˜ ìœ„ì¹˜ì— ë²ˆê°ˆì•„ ì ìš©í•˜ê¸° ìœ„í•¨\n",
    "        # exp(-log(10000) * 2i/d_model) = 10000^(-2i/d_model) = 1/10000^(2i/d_model)\n",
    "        # ì´ìœ : ë‚®ì€ ì°¨ì›ì—ì„œëŠ” ë¹ ë¥´ê²Œ ë³€í•˜ê³ , ë†’ì€ ì°¨ì›ì—ì„œëŠ” ì²œì²œíˆ ë³€í•˜ëŠ” ì£¼íŒŒìˆ˜ ìƒì„±\n",
    "        #       -> ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ í¬ì°©í•  ìˆ˜ ìˆìŒ\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # sin í•¨ìˆ˜ ì ìš©: ì§ìˆ˜ ì¸ë±ìŠ¤ (0, 2, 4, ...)\n",
    "        # pe[:, 0::2]: ëª¨ë“  í–‰ì˜ ì§ìˆ˜ ì—´ ì„ íƒ\n",
    "        # position * div_term: (max_len, 1) * (d_model/2,) -> (max_len, d_model/2)\n",
    "        # ì´ìœ : ê° ìœ„ì¹˜ë§ˆë‹¤ ë‹¤ë¥¸ ì£¼íŒŒìˆ˜ì˜ sin í•¨ìˆ˜ê°’ì„ ê³„ì‚°\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # cos í•¨ìˆ˜ ì ìš©: í™€ìˆ˜ ì¸ë±ìŠ¤ (1, 3, 5, ...)\n",
    "        # ì´ìœ : sinê³¼ cosë¥¼ ë²ˆê°ˆì•„ ì‚¬ìš©í•˜ë©´ ìœ„ì¹˜ë¥¼ ë” ëª…í™•í•˜ê²Œ êµ¬ë¶„ ê°€ëŠ¥\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: (max_len, d_model) -> (1, max_len, d_model)\n",
    "        # ì´ìœ : ì…ë ¥ í…ì„œê°€ (batch_size, seq_len, d_model) í˜•íƒœì´ë¯€ë¡œ \n",
    "        #       broadcastingì„ ìœ„í•´ ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # register_buffer: ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ëŠ” ì•„ë‹ˆì§€ë§Œ ì €ì¥ì´ í•„ìš”í•œ í…ì„œ ë“±ë¡\n",
    "        # ì´ìœ : 1) í•™ìŠµë˜ì§€ ì•ŠëŠ” ê³ ì •ê°’ 2) model.save() ì‹œ í•¨ê»˜ ì €ì¥ë¨ \n",
    "        #       3) GPU/CPU ì´ë™ ì‹œ ìë™ìœ¼ë¡œ í•¨ê»˜ ì´ë™\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): ì…ë ¥ ì„ë² ë”©, shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: ìœ„ì¹˜ ì •ë³´ê°€ ì¶”ê°€ëœ ì„ë² ë”©, shape: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # ì…ë ¥ xì— ìœ„ì¹˜ ì •ë³´ ë”í•˜ê¸°\n",
    "        # self.pe[:, :x.size(1)]: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ë§Œ ì˜ë¼ì„œ ì‚¬ìš©\n",
    "        # ì´ìœ : ì…ë ¥ ë¬¸ì¥ì´ max_lenë³´ë‹¤ ì§§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•„ìš”í•œ ê¸¸ì´ë§Œí¼ë§Œ ë”í•¨\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì½”ë“œ\n",
    "print(\"=\"*60)\n",
    "print(\"Positional Encoding í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© Positional Encoding ë ˆì´ì–´ ìƒì„±\n",
    "# d_model=8: ì‘ì€ ì°¨ì›ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œë¡œëŠ” 512 ì •ë„ ì‚¬ìš©)\n",
    "pe_layer = PositionalEncoding(d_model=8, max_len=10)\n",
    "\n",
    "# ëœë¤ ìƒ˜í”Œ ì…ë ¥ ìƒì„±: (batch=2, seq_len=5, d_model=8)\n",
    "# randn: í‘œì¤€ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•œ ëœë¤ í…ì„œ\n",
    "sample_input = torch.randn(2, 5, 8)\n",
    "\n",
    "# forward ì‹¤í–‰: ìœ„ì¹˜ ì •ë³´ê°€ ì¶”ê°€ëœ ì¶œë ¥\n",
    "output = pe_layer(sample_input)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(f\"ì…ë ¥ shape: {sample_input.shape}\")  # (2, 5, 8)\n",
    "print(f\"ì¶œë ¥ shape: {output.shape}\")        # (2, 5, 8) - shapeëŠ” ë™ì¼\n",
    "print(f\"âœ“ Positional Encoding ì™„ë£Œ!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2"
   },
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£ ë‹¨ê³„ 2: Multi-Head Attention (ë©€í‹°í—¤ë“œ ì–´í…ì…˜)\n",
    "\n",
    "ì—¬ëŸ¬ ê°œì˜ attention headë¥¼ ì‚¬ìš©í•˜ì—¬ **ë‹¤ì–‘í•œ ê´€ì **ì—ì„œ ë‹¨ì–´ ê°„ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "### í•µì‹¬ ê³µì‹\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### ì˜ˆì‹œ\n",
    "ë¬¸ì¥: \"ë‚˜ëŠ” í•™êµì— ê°„ë‹¤\"\n",
    "- ì–´ë–¤ headëŠ” **ì£¼ì–´-ë™ì‚¬** ê´€ê³„ì— ì§‘ì¤‘\n",
    "- ë‹¤ë¥¸ headëŠ” **ëª©ì ì–´-ë™ì‚¬** ê´€ê³„ì— ì§‘ì¤‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "multihead_attention"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention í´ë˜ìŠ¤\n",
    "    \n",
    "    ì—­í• : ì—¬ëŸ¬ ê°œì˜ ì–´í…ì…˜ í—¤ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ë³‘ë ¬ë¡œ í•™ìŠµ\n",
    "    ì´ìœ : ë‹¨ì¼ ì–´í…ì…˜ë³´ë‹¤ ì—¬ëŸ¬ ê´€ì ì—ì„œ ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆì–´ ì„±ëŠ¥ì´ ì¢‹ìŒ\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): ëª¨ë¸ì˜ ì°¨ì› (ì˜ˆ: 512)\n",
    "            num_heads (int): ì–´í…ì…˜ í—¤ë“œ ê°œìˆ˜ (ì˜ˆ: 8)\n",
    "        \n",
    "        ì´ìœ : d_modelì„ num_headsë¡œ ë‚˜ëˆ„ì–´ ê° í—¤ë“œê°€ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # d_modelì´ num_headsë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ëŠ”ì§€ í™•ì¸\n",
    "        # ì´ìœ : ê° í—¤ë“œê°€ ë™ì¼í•œ ì°¨ì›ì„ ê°€ì ¸ì•¼ í•˜ë¯€ë¡œ\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # ê° í—¤ë“œì˜ ì°¨ì› ê³„ì‚° (ì˜ˆ: 512 / 8 = 64)\n",
    "        # ì´ìœ : ì „ì²´ ì°¨ì›ì„ í—¤ë“œ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ê° í—¤ë“œê°€ ë‹´ë‹¹í•  ì°¨ì› ê²°ì •\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Query, Key, Valueë¥¼ ë§Œë“œëŠ” ì„ í˜• ë³€í™˜ ë ˆì´ì–´\n",
    "        # Linear(d_model, d_model): ì…ë ¥ê³¼ ì¶œë ¥ ì°¨ì›ì´ ê°™ìŒ\n",
    "        # ì´ìœ : ì…ë ¥ì„ Q, K, Vë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ í–‰ë ¬\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # ëª¨ë“  í—¤ë“œì˜ ì¶œë ¥ì„ ê²°í•©í•œ í›„ ìµœì¢… ë³€í™˜í•˜ëŠ” ë ˆì´ì–´\n",
    "        # ì´ìœ : ê° í—¤ë“œì˜ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ìµœì¢… ì¶œë ¥ ìƒì„±\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Scaled Dot-Product Attention ê³„ì‚°\n",
    "        \n",
    "        ì—­í• : Q(Query)ì™€ K(Key)ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ V(Value)ì— ê°€ì¤‘ì¹˜ ì ìš©\n",
    "        \n",
    "        ì§ê´€ì  ì„¤ëª…:\n",
    "        1. Qì™€ Kë¥¼ ê³±í•´ì„œ ìœ ì‚¬ë„ ê³„ì‚° (ë‚´ì ì´ í´ìˆ˜ë¡ ìœ ì‚¬)\n",
    "        2. softmaxë¡œ í™•ë¥  ë¶„í¬ ë³€í™˜ (í•©ì´ 1ì´ ë˜ë„ë¡)\n",
    "        3. Vì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ì„œ ìµœì¢… ì¶œë ¥ (ì¤‘ìš”í•œ ì •ë³´ì— ì§‘ì¤‘)\n",
    "        \n",
    "        Args:\n",
    "            Q (Tensor): Query, shape: (batch, num_heads, seq_len, d_k)\n",
    "            K (Tensor): Key, shape: (batch, num_heads, seq_len, d_k)\n",
    "            V (Tensor): Value, shape: (batch, num_heads, seq_len, d_k)\n",
    "            mask (Tensor): ì–´í…ì…˜ ë§ˆìŠ¤í¬ (ì„ íƒì )\n",
    "        \n",
    "        Returns:\n",
    "            output (Tensor): ì–´í…ì…˜ ì ìš© ê²°ê³¼\n",
    "            attention_weights (Tensor): ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ì‹œê°í™” ê°€ëŠ¥)\n",
    "        \"\"\"\n",
    "        # Qì™€ Kì˜ ë‚´ì ìœ¼ë¡œ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°\n",
    "        # K.transpose(-2, -1): Kì˜ ë§ˆì§€ë§‰ ë‘ ì°¨ì›ì„ ì „ì¹˜ (seq_len, d_k) -> (d_k, seq_len)\n",
    "        # matmul(Q, K^T): (batch, num_heads, seq_len_q, d_k) x (batch, num_heads, d_k, seq_len_k)\n",
    "        #                 -> (batch, num_heads, seq_len_q, seq_len_k)\n",
    "        # ì´ìœ : ê° Queryê°€ ëª¨ë“  Keyì™€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ê³„ì‚°\n",
    "        \n",
    "        # math.sqrt(self.d_k)ë¡œ ë‚˜ëˆ„ê¸° (Scaling)\n",
    "        # ì´ìœ : 1) ë‚´ì  ê°’ì´ ë„ˆë¬´ ì»¤ì§€ë©´ softmaxì˜ gradientê°€ ì†Œì‹¤ë  ìˆ˜ ìˆìŒ\n",
    "        #       2) d_kê°€ í´ìˆ˜ë¡ ë‚´ì  ê°’ì˜ ë¶„ì‚°ì´ ì»¤ì§€ë¯€ë¡œ âˆšd_kë¡œ ì •ê·œí™”\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # ë§ˆìŠ¤í¬ ì ìš© (ì„ íƒì )\n",
    "        # mask == 0ì¸ ìœ„ì¹˜ë¥¼ -1e9(ìŒì˜ ë¬´í•œëŒ€ì— ê°€ê¹Œìš´ ê°’)ë¡œ ì„¤ì •\n",
    "        # ì´ìœ : 1) Decoderì—ì„œ ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³´ì§€ ëª»í•˜ê²Œ í•¨ (causal masking)\n",
    "        #       2) padding í† í°ì„ ë¬´ì‹œ (padding mask)\n",
    "        #       3) softmax í›„ ì´ ìœ„ì¹˜ì˜ í™•ë¥ ì´ ê±°ì˜ 0ì´ ë¨\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # softmaxë¡œ attention ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "        # dim=-1: ë§ˆì§€ë§‰ ì°¨ì›(seq_len_k)ì— ëŒ€í•´ softmax ì ìš©\n",
    "        # ì´ìœ : ê° Queryì— ëŒ€í•´ ëª¨ë“  Keyì˜ ê°€ì¤‘ì¹˜ í•©ì´ 1ì´ ë˜ë„ë¡\n",
    "        #       -> í™•ë¥  ë¶„í¬ë¡œ í•´ì„ ê°€ëŠ¥\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ë¥¼ Vì— ê³±í•˜ê¸° (ê°€ì¤‘ í‰ê· )\n",
    "        # (batch, num_heads, seq_len_q, seq_len_k) x (batch, num_heads, seq_len_k, d_k)\n",
    "        # -> (batch, num_heads, seq_len_q, d_k)\n",
    "        # ì´ìœ : ì¤‘ìš”í•œ Valueì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ ì •ë³´ ì¶”ì¶œ\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Multi-Head Attentionì˜ forward pass\n",
    "        \n",
    "        Args:\n",
    "            query, key, value (Tensor): shape: (batch_size, seq_len, d_model)\n",
    "            mask (Tensor): ì–´í…ì…˜ ë§ˆìŠ¤í¬ (ì„ íƒì )\n",
    "        \n",
    "        Returns:\n",
    "            output (Tensor): ìµœì¢… ì¶œë ¥, shape: (batch_size, seq_len, d_model)\n",
    "            attention_weights (Tensor): ì–´í…ì…˜ ê°€ì¤‘ì¹˜\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1ë‹¨ê³„: Linear ë³€í™˜ í›„ multi-headë¡œ split\n",
    "        # Q = W_q * query: (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        # viewë¡œ reshape: (batch, seq_len, num_heads, d_k)\n",
    "        # ì´ìœ : d_model ì°¨ì›ì„ (num_heads, d_k)ë¡œ ë¶„í• í•˜ì—¬ ê° í—¤ë“œê°€ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬\n",
    "        # transpose(1, 2): (batch, seq_len, num_heads, d_k) -> (batch, num_heads, seq_len, d_k)\n",
    "        # ì´ìœ : í—¤ë“œ ì°¨ì›ì„ ì•ìœ¼ë¡œ ê°€ì ¸ì™€ ë³‘ë ¬ ì²˜ë¦¬ ìš©ì´\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2ë‹¨ê³„: Scaled Dot-Product Attention ê³„ì‚°\n",
    "        # ê° í—¤ë“œê°€ ë…ë¦½ì ìœ¼ë¡œ ì–´í…ì…˜ ê³„ì‚°\n",
    "        x, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 3ë‹¨ê³„: Multi-head ê²°ê³¼ í•©ì¹˜ê¸° (Concatenation)\n",
    "        # transpose(1, 2): (batch, num_heads, seq_len, d_k) -> (batch, seq_len, num_heads, d_k)\n",
    "        # ì´ìœ : ì›ë˜ ìˆœì„œë¡œ ë˜ëŒë¦¼\n",
    "        # contiguous(): ë©”ëª¨ë¦¬ ìƒì—ì„œ ì—°ì†ëœ ë°°ì¹˜ë¡œ ë§Œë“¦\n",
    "        # ì´ìœ : view ì—°ì‚°ì„ ìœ„í•´ í•„ìš” (transpose í›„ ë©”ëª¨ë¦¬ê°€ ë¹„ì—°ì†ì ì¼ ìˆ˜ ìˆìŒ)\n",
    "        # view(batch_size, -1, self.d_model): (batch, seq_len, num_heads * d_k)\n",
    "        # ì´ìœ : ëª¨ë“  í—¤ë“œì˜ ì¶œë ¥ì„ í•˜ë‚˜ë¡œ í•©ì¹¨ (concat)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 4ë‹¨ê³„: ìµœì¢… linear ë³€í™˜\n",
    "        # ì´ìœ : ê²°í•©ëœ í—¤ë“œ ì •ë³´ë¥¼ ë‹¤ì‹œ d_model ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ \n",
    "        #       ë‹¤ìŒ ë ˆì´ì–´ë¡œ ì „ë‹¬í•  í‘œí˜„ ìƒì„±\n",
    "        output = self.W_o(x)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì½”ë“œ\n",
    "print(\"=\"*60)\n",
    "print(\"Multi-Head Attention í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Multi-Head Attention ë ˆì´ì–´ ìƒì„±\n",
    "mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "\n",
    "# ëœë¤ ìƒ˜í”Œ ì…ë ¥ ìƒì„±: (batch=2, seq_len=10, d_model=512)\n",
    "sample_input = torch.randn(2, 10, 512)\n",
    "\n",
    "# Self-Attention ìˆ˜í–‰: query, key, value ëª¨ë‘ ê°™ì€ ì…ë ¥ ì‚¬ìš©\n",
    "# ì´ìœ : ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ë¼ë¦¬ì˜ ê´€ê³„ë¥¼ í•™ìŠµ (ìê¸° ìì‹ ì„ ì°¸ì¡°)\n",
    "output, attn_weights = mha(sample_input, sample_input, sample_input)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(f\"ì…ë ¥ shape: {sample_input.shape}\")           # (2, 10, 512)\n",
    "print(f\"ì¶œë ¥ shape: {output.shape}\")                 # (2, 10, 512) - shape ë™ì¼\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")  # (2, 8, 10, 10)\n",
    "#     (batch, num_heads, seq_len, seq_len)\n",
    "print(f\"âœ“ Multi-Head Attention ì™„ë£Œ!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3"
   },
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£ ë‹¨ê³„ 3: Feed Forward Network\n",
    "\n",
    "ê°„ë‹¨í•œ 2ì¸µ ì‹ ê²½ë§ì…ë‹ˆë‹¤.  \n",
    "ê° ë‹¨ì–´ í‘œí˜„ì„ **ë…ë¦½ì ìœ¼ë¡œ** ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feedforward"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed Forward Network (Position-wise)\n",
    "    \n",
    "    ì—­í• : ê° ìœ„ì¹˜(ë‹¨ì–´)ì˜ í‘œí˜„ì„ ë…ë¦½ì ìœ¼ë¡œ ë¹„ì„ í˜• ë³€í™˜\n",
    "    ì´ìœ : 1) ì–´í…ì…˜ì€ ì„ í˜• ë³€í™˜ì´ë¯€ë¡œ ë¹„ì„ í˜•ì„±ì„ ì¶”ê°€í•˜ì—¬ í‘œí˜„ë ¥ í–¥ìƒ\n",
    "          2) ê° ë‹¨ì–´ì˜ í‘œí˜„ì„ ë” í’ë¶€í•˜ê²Œ ë§Œë“¦\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): ì…ë ¥/ì¶œë ¥ ì°¨ì› (ì˜ˆ: 512)\n",
    "            d_ff (int): ì¤‘ê°„ hidden layer ì°¨ì› (ì˜ˆ: 2048)\n",
    "        \n",
    "        ì´ìœ : d_ffëŠ” ë³´í†µ d_modelì˜ 4ë°° ì •ë„ë¡œ ì„¤ì •\n",
    "              ë” í° ì°¨ì›ì—ì„œ ë³€í™˜ í›„ ë‹¤ì‹œ ì¶•ì†Œí•˜ì—¬ í‘œí˜„ë ¥ ì¦ê°€\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ ì„ í˜• ë ˆì´ì–´: d_model -> d_ffë¡œ ì°¨ì› í™•ì¥\n",
    "        # ì´ìœ : ë” í° ê³µê°„ì—ì„œ ë³µì¡í•œ ë³€í™˜ ìˆ˜í–‰\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        \n",
    "        # ë‘ ë²ˆì§¸ ì„ í˜• ë ˆì´ì–´: d_ff -> d_modelë¡œ ì°¨ì› ì¶•ì†Œ\n",
    "        # ì´ìœ : ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë˜ëŒë ¤ ë‹¤ìŒ ë ˆì´ì–´ë¡œ ì „ë‹¬\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        # ReLU í™œì„±í™” í•¨ìˆ˜: max(0, x)\n",
    "        # ì´ìœ : ë¹„ì„ í˜•ì„± ë„ì… (ìŒìˆ˜ëŠ” 0ìœ¼ë¡œ, ì–‘ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ)\n",
    "        #       ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ í™œì„±í™” í•¨ìˆ˜\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): ì…ë ¥, shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: ì¶œë ¥, shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        ê³¼ì •:\n",
    "        1. linear1: (batch, seq_len, d_model) -> (batch, seq_len, d_ff)\n",
    "        2. relu: ìŒìˆ˜ ê°’ì„ 0ìœ¼ë¡œ (ë¹„ì„ í˜• ë³€í™˜)\n",
    "        3. linear2: (batch, seq_len, d_ff) -> (batch, seq_len, d_model)\n",
    "        \n",
    "        ì¤‘ìš”: ê° ìœ„ì¹˜(seq_len)ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ê°™ì€ ë³€í™˜ ì ìš© (Position-wise)\n",
    "        ì´ìœ : ë‹¨ì–´ë“¤ ê°„ì˜ ìƒí˜¸ì‘ìš©ì€ ì–´í…ì…˜ì—ì„œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ\n",
    "              ì—¬ê¸°ì„œëŠ” ê° ë‹¨ì–´ì˜ í‘œí˜„ë§Œ ê°œì„ \n",
    "        \"\"\"\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì½”ë“œ\n",
    "print(\"=\"*60)\n",
    "print(\"Feed Forward Network í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# FFN ë ˆì´ì–´ ìƒì„±\n",
    "ffn = FeedForward(d_model=512, d_ff=2048)\n",
    "\n",
    "# ëœë¤ ìƒ˜í”Œ ì…ë ¥\n",
    "sample_input = torch.randn(2, 10, 512)\n",
    "output = ffn(sample_input)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(f\"ì…ë ¥ shape: {sample_input.shape}\")  # (2, 10, 512)\n",
    "print(f\"ì¶œë ¥ shape: {output.shape}\")        # (2, 10, 512) - shape ë™ì¼\n",
    "print(f\"âœ“ Feed Forward Network ì™„ë£Œ!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4"
   },
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£ ë‹¨ê³„ 4: Encoder Layer (ì¸ì½”ë” ë ˆì´ì–´)\n",
    "\n",
    "Transformer Encoderì˜ ê¸°ë³¸ ë¸”ë¡ì…ë‹ˆë‹¤.\n",
    "\n",
    "### êµ¬ì¡°\n",
    "1. Multi-Head **Self-Attention**\n",
    "2. Add & Norm (Residual Connection + Layer Normalization)\n",
    "3. Feed Forward Network\n",
    "4. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "encoder_layer"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Layer\n",
    "    \n",
    "    ì—­í• : ì…ë ¥ ë¬¸ì¥ì„ ì´í•´í•˜ê³  ì˜ë¯¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¸ì½”ë”ì˜ í•œ ì¸µ\n",
    "    êµ¬ì¡°: Self-Attention -> Add&Norm -> FFN -> Add&Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): ëª¨ë¸ ì°¨ì›\n",
    "            num_heads (int): ì–´í…ì…˜ í—¤ë“œ ê°œìˆ˜\n",
    "            d_ff (int): FFNì˜ hidden ì°¨ì›\n",
    "            dropout (float): Dropout ë¹„ìœ¨ (ê¸°ë³¸ 0.1 = 10%)\n",
    "        \n",
    "        dropout ì´ìœ : ê³¼ì í•©(overfitting) ë°©ì§€\n",
    "                     í•™ìŠµ ì‹œ ëœë¤í•˜ê²Œ ì¼ë¶€ ë‰´ëŸ°ì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´ \n",
    "                     ëª¨ë¸ì´ íŠ¹ì • ë‰´ëŸ°ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ì§€ ì•Šë„ë¡ í•¨\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Self-Attention\n",
    "        # ì´ìœ : ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„ í•™ìŠµ\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed Forward Network\n",
    "        # ì´ìœ : ê° ë‹¨ì–´ì˜ í‘œí˜„ì„ ë¹„ì„ í˜• ë³€í™˜\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        # Layer Normalization (ì •ê·œí™”)\n",
    "        # LayerNorm: ê° ìƒ˜í”Œì˜ featureë“¤ì„ ì •ê·œí™” (í‰ê·  0, ë¶„ì‚° 1)\n",
    "        # ì´ìœ : 1) í•™ìŠµ ì•ˆì •í™” (gradient í­ë°œ/ì†Œì‹¤ ë°©ì§€)\n",
    "        #       2) í•™ìŠµ ì†ë„ í–¥ìƒ\n",
    "        #       3) ì´ˆê¸°í™”ì— ëœ ë¯¼ê°\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout ë ˆì´ì–´\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): ì…ë ¥, shape: (batch_size, seq_len, d_model)\n",
    "            mask (Tensor): ì–´í…ì…˜ ë§ˆìŠ¤í¬ (padding ë¬´ì‹œìš©)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: ì¸ì½”ë” ë ˆì´ì–´ ì¶œë ¥, shape: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Self-Attention + Residual Connection + Normalization\n",
    "        # Self-Attention: query, key, value ëª¨ë‘ x ì‚¬ìš©\n",
    "        # ì´ìœ : ì…ë ¥ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ì˜ ê´€ê³„ íŒŒì•…\n",
    "        attn_output, _ = self.attention(x, x, x, mask)\n",
    "        \n",
    "        # Residual Connection: x + dropout(attn_output)\n",
    "        # ì´ìœ : 1) Gradient íë¦„ ê°œì„  (ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥)\n",
    "        #       2) ì›ë³¸ ì •ë³´ ë³´ì¡´ (ì •ë³´ ì†ì‹¤ ë°©ì§€)\n",
    "        # Layer Normalization: ì •ê·œí™”í•˜ì—¬ ì•ˆì •ì ì¸ í•™ìŠµ\n",
    "        # ìˆœì„œ: x + dropout(sublayer(x)) -> LayerNorm (Post-LN)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # 2. Feed Forward + Residual Connection + Normalization\n",
    "        # FFN: ê° ìœ„ì¹˜ì˜ í‘œí˜„ì„ ë…ë¦½ì ìœ¼ë¡œ ë³€í™˜\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # ë‹¤ì‹œ Residual Connectionê³¼ LayerNorm ì ìš©\n",
    "        # ì´ìœ : ìœ„ì™€ ë™ì¼ (gradient íë¦„, ì •ë³´ ë³´ì¡´)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì½”ë“œ\n",
    "print(\"=\"*60)\n",
    "print(\"Encoder Layer í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Encoder Layer ìƒì„±\n",
    "encoder_layer = EncoderLayer(d_model=512, num_heads=8, d_ff=2048)\n",
    "\n",
    "# ëœë¤ ìƒ˜í”Œ ì…ë ¥\n",
    "sample_input = torch.randn(2, 10, 512)\n",
    "output = encoder_layer(sample_input)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(f\"ì…ë ¥ shape: {sample_input.shape}\")  # (2, 10, 512)\n",
    "print(f\"ì¶œë ¥ shape: {output.shape}\")        # (2, 10, 512) - shape ë™ì¼\n",
    "print(f\"âœ“ Encoder Layer ì™„ë£Œ!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5"
   },
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£ ë‹¨ê³„ 5: Decoder Layer (ë””ì½”ë” ë ˆì´ì–´)\n",
    "\n",
    "Transformer Decoderì˜ ê¸°ë³¸ ë¸”ë¡ì…ë‹ˆë‹¤.\n",
    "\n",
    "### êµ¬ì¡°\n",
    "1. Masked Multi-Head **Self-Attention** (ìê¸° ìì‹ )\n",
    "2. Add & Norm\n",
    "3. Multi-Head **Cross-Attention** (Encoder ì¶œë ¥ ì°¸ì¡°)\n",
    "4. Add & Norm\n",
    "5. Feed Forward Network\n",
    "6. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "decoder_layer"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Layer\n",
    "    \n",
    "    ì—­í• : ì¸ì½”ë”ì˜ ì¶œë ¥ì„ ì°¸ì¡°í•˜ì—¬ ì¶œë ¥ ë¬¸ì¥ì„ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±\n",
    "    êµ¬ì¡°: Masked Self-Attention -> Add&Norm -> Cross-Attention -> Add&Norm -> FFN -> Add&Norm\n",
    "    \n",
    "    Encoderì™€ì˜ ì°¨ì´ì :\n",
    "    1. Masked Self-Attention: ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³¼ ìˆ˜ ì—†ìŒ (autoregressive)\n",
    "    2. Cross-Attention: ì¸ì½”ë” ì¶œë ¥ì„ ì°¸ì¡°í•˜ì—¬ ì…ë ¥ ë¬¸ì¥ ì •ë³´ í™œìš©\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): ëª¨ë¸ ì°¨ì›\n",
    "            num_heads (int): ì–´í…ì…˜ í—¤ë“œ ê°œìˆ˜\n",
    "            d_ff (int): FFNì˜ hidden ì°¨ì›\n",
    "            dropout (float): Dropout ë¹„ìœ¨\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-Attention: ì¶œë ¥ ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„\n",
    "        # ì´ìœ : ì§€ê¸ˆê¹Œì§€ ìƒì„±í•œ ë‹¨ì–´ë“¤ì„ ì°¸ì¡°í•˜ì—¬ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Cross-Attention: ì¸ì½”ë” ì¶œë ¥ì„ ì°¸ì¡°\n",
    "        # ì´ìœ : ì…ë ¥ ë¬¸ì¥(source)ì˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì¶œë ¥(target) ìƒì„±\n",
    "        #       ì˜ˆ: \"I love you\" -> \"ë‚˜ëŠ” ë„ˆë¥¼ ì‚¬ë‘í•´\" ë²ˆì—­ ì‹œ\n",
    "        #           \"ì‚¬ë‘í•´\"ë¥¼ ìƒì„±í•  ë•Œ \"love\"ì— ì£¼ëª©\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed Forward Network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        # Layer Normalization (3ê°œ í•„ìš”)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): ë””ì½”ë” ì…ë ¥ (target), shape: (batch, tgt_len, d_model)\n",
    "            encoder_output (Tensor): ì¸ì½”ë” ì¶œë ¥, shape: (batch, src_len, d_model)\n",
    "            src_mask (Tensor): ì†ŒìŠ¤ ë§ˆìŠ¤í¬ (padding ë¬´ì‹œ)\n",
    "            tgt_mask (Tensor): íƒ€ê²Ÿ ë§ˆìŠ¤í¬ (padding + ë¯¸ë˜ ë‹¨ì–´ ë¬´ì‹œ)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: ë””ì½”ë” ë ˆì´ì–´ ì¶œë ¥, shape: (batch, tgt_len, d_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Masked Self-Attention\n",
    "        # tgt_mask ì ìš©: í˜„ì¬ ìœ„ì¹˜ë³´ë‹¤ ë’¤ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ë³¼ ìˆ˜ ì—†ìŒ\n",
    "        # ì´ìœ : í•™ìŠµ ì‹œ ì •ë‹µì„ ë¯¸ë¦¬ ë³´ë©´ ì•ˆ ë˜ë¯€ë¡œ (Teacher Forcing)\n",
    "        #       ì¶”ë¡  ì‹œì—ë„ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ë¯€ë¡œ ë¯¸ë˜ ë‹¨ì–´ê°€ ì—†ìŒ\n",
    "        attn_output, _ = self.self_attention(x, x, x, tgt_mask)\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # 2. Cross-Attention (í•µì‹¬!)\n",
    "        # Query: ë””ì½”ë”ì˜ í˜„ì¬ ìƒíƒœ (x)\n",
    "        # Key, Value: ì¸ì½”ë” ì¶œë ¥ (encoder_output)\n",
    "        # ì´ìœ : \"ë””ì½”ë”ê°€ í˜„ì¬ ìƒì„±í•  ë‹¨ì–´ë¥¼ ìœ„í•´ ì…ë ¥ ë¬¸ì¥ì˜ ì–´ë””ì— ì£¼ëª©í•´ì•¼ í•˜ëŠ”ê°€?\"\n",
    "        #       ì˜ˆ: \"ë‚˜ëŠ”\"ì„ ìƒì„±í•  ë•Œ \"I\"ì—, \"ì‚¬ë‘í•´\"ë¥¼ ìƒì„±í•  ë•Œ \"love\"ì— ì£¼ëª©\n",
    "        attn_output, _ = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # 3. Feed Forward Network\n",
    "        # ê° ìœ„ì¹˜ì˜ í‘œí˜„ì„ ë…ë¦½ì ìœ¼ë¡œ ë³€í™˜\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì½”ë“œ\n",
    "print(\"=\"*60)\n",
    "print(\"Decoder Layer í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Decoder Layer ìƒì„±\n",
    "decoder_layer = DecoderLayer(d_model=512, num_heads=8, d_ff=2048)\n",
    "\n",
    "# ëœë¤ ìƒ˜í”Œ ì…ë ¥\n",
    "# tgt_input: ë””ì½”ë” ì…ë ¥ (ì¶œë ¥ ë¬¸ì¥)\n",
    "# enc_output: ì¸ì½”ë” ì¶œë ¥ (ì…ë ¥ ë¬¸ì¥ì˜ í‘œí˜„)\n",
    "tgt_input = torch.randn(2, 8, 512)   # ì¶œë ¥ ë¬¸ì¥ ê¸¸ì´ 8\n",
    "enc_output = torch.randn(2, 10, 512)  # ì…ë ¥ ë¬¸ì¥ ê¸¸ì´ 10\n",
    "\n",
    "output = decoder_layer(tgt_input, enc_output)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(f\"Target ì…ë ¥ shape: {tgt_input.shape}\")     # (2, 8, 512)\n",
    "print(f\"Encoder ì¶œë ¥ shape: {enc_output.shape}\")  # (2, 10, 512)\n",
    "print(f\"Decoder ì¶œë ¥ shape: {output.shape}\")      # (2, 8, 512)\n",
    "print(f\"âœ“ Decoder Layer ì™„ë£Œ!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6"
   },
   "source": [
    "---\n",
    "\n",
    "## 6ï¸âƒ£ ë‹¨ê³„ 6: ì™„ì „í•œ Transformer ëª¨ë¸\n",
    "\n",
    "ì´ì œ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ ê²°í•©í•˜ì—¬ **ì™„ì „í•œ Transformer**ë¥¼ ë§Œë“­ë‹ˆë‹¤!\n",
    "\n",
    "### ì „ì²´ êµ¬ì¡°\n",
    "```\n",
    "Input â†’ Embedding â†’ Positional Encoding â†’ Encoder (Ã—N) â†’ \n",
    "                                              â†“\n",
    "Target â†’ Embedding â†’ Positional Encoding â†’ Decoder (Ã—N) â†’ Linear â†’ Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transformer"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    ì™„ì „í•œ Transformer ëª¨ë¸ (Sequence-to-Sequence)\n",
    "    \n",
    "    ì—­í• : ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì¶œë ¥ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜ (ì˜ˆ: ê¸°ê³„ ë²ˆì—­)\n",
    "    êµ¬ì¡°: Encoder (ì…ë ¥ ì´í•´) + Decoder (ì¶œë ¥ ìƒì„±)\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, \n",
    "                 num_heads=8, num_layers=6, d_ff=2048, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_vocab_size (int): ì†ŒìŠ¤(ì…ë ¥) ì–¸ì–´ì˜ ì–´íœ˜ í¬ê¸°\n",
    "            tgt_vocab_size (int): íƒ€ê²Ÿ(ì¶œë ¥) ì–¸ì–´ì˜ ì–´íœ˜ í¬ê¸°\n",
    "            d_model (int): ëª¨ë¸ì˜ ì°¨ì› (ê¸°ë³¸ 512)\n",
    "            num_heads (int): ì–´í…ì…˜ í—¤ë“œ ê°œìˆ˜ (ê¸°ë³¸ 8)\n",
    "            num_layers (int): ì¸ì½”ë”/ë””ì½”ë” ë ˆì´ì–´ ê°œìˆ˜ (ê¸°ë³¸ 6)\n",
    "            d_ff (int): FFNì˜ hidden ì°¨ì› (ê¸°ë³¸ 2048)\n",
    "            dropout (float): Dropout ë¹„ìœ¨ (ê¸°ë³¸ 0.1)\n",
    "            max_len (int): ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ 5000)\n",
    "        \n",
    "        ë…¼ë¬¸ \"Attention is All You Need\"ì˜ ê¸°ë³¸ ì„¤ì •:\n",
    "        d_model=512, num_heads=8, num_layers=6, d_ff=2048\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        # nn.Embedding: ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ê³ ì • í¬ê¸° ë²¡í„°ë¡œ ë³€í™˜\n",
    "        # ì˜ˆ: ë‹¨ì–´ ID 5 -> [0.2, -0.1, 0.5, ..., 0.3] (d_model ì°¨ì› ë²¡í„°)\n",
    "        # ì´ìœ : ë‹¨ì–´ë¥¼ ì»´í“¨í„°ê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜ ë²¡í„°ë¡œ í‘œí˜„\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        # ì´ìœ : ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ ì¶”ê°€ (\"ì²« ë²ˆì§¸ ë‹¨ì–´\", \"ë‘ ë²ˆì§¸ ë‹¨ì–´\" ë“±)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Encoder layers (ì—¬ëŸ¬ ì¸µì„ ìŒ“ìŒ)\n",
    "        # nn.ModuleList: ì—¬ëŸ¬ ë ˆì´ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬ (íŒŒë¼ë¯¸í„° ìë™ ë“±ë¡)\n",
    "        # ì´ìœ : ê°™ì€ êµ¬ì¡°ì˜ ë ˆì´ì–´ë¥¼ num_layersê°œ ìŒ“ì•„ì„œ \n",
    "        #       ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ (ê¹Šì€ ë„¤íŠ¸ì›Œí¬)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder layers (ì—¬ëŸ¬ ì¸µì„ ìŒ“ìŒ)\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # ìµœì¢… ì¶œë ¥ ë ˆì´ì–´\n",
    "        # Linear: d_model -> tgt_vocab_size\n",
    "        # ì´ìœ : ê° ìœ„ì¹˜ë§ˆë‹¤ íƒ€ê²Ÿ ì–´íœ˜ì˜ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•œ ì ìˆ˜(logit) ê³„ì‚°\n",
    "        #       ì˜ˆ: [0.1, 2.3, -0.5, ...] (vocab_size ê°œì˜ ì ìˆ˜)\n",
    "        #       ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # d_model ì €ì¥ (embedding scalingì— ì‚¬ìš©)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def generate_mask(self, src, tgt):\n",
    "        \"\"\"\n",
    "        ë§ˆìŠ¤í¬ ìƒì„± í•¨ìˆ˜\n",
    "        \n",
    "        ì—­í• : 1) Padding í† í° ë¬´ì‹œ 2) ë¯¸ë˜ ë‹¨ì–´ ê°€ë¦¬ê¸°\n",
    "        \n",
    "        Args:\n",
    "            src (Tensor): ì†ŒìŠ¤ ì‹œí€€ìŠ¤, shape: (batch, src_len)\n",
    "            tgt (Tensor): íƒ€ê²Ÿ ì‹œí€€ìŠ¤, shape: (batch, tgt_len)\n",
    "        \n",
    "        Returns:\n",
    "            src_mask: ì†ŒìŠ¤ ë§ˆìŠ¤í¬ (batch, 1, 1, src_len)\n",
    "            tgt_mask: íƒ€ê²Ÿ ë§ˆìŠ¤í¬ (batch, 1, tgt_len, tgt_len)\n",
    "        \"\"\"\n",
    "        # Source Mask: padding í† í°(0) ìœ„ì¹˜ë¥¼ Falseë¡œ\n",
    "        # (src != 0): paddingì´ ì•„ë‹Œ ìœ„ì¹˜ëŠ” True\n",
    "        # unsqueeze(1).unsqueeze(2): broadcastingì„ ìœ„í•œ ì°¨ì› ì¶”ê°€\n",
    "        # ì´ìœ : ì–´í…ì…˜ì—ì„œ padding ìœ„ì¹˜ë¥¼ ë¬´ì‹œí•˜ê¸° ìœ„í•¨\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # Target Mask: padding + ë¯¸ë˜ ë‹¨ì–´ ê°€ë¦¬ê¸°\n",
    "        tgt_len = tgt.size(1)\n",
    "        \n",
    "        # Padding mask\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        \n",
    "        # Subsequent mask (í•˜ì‚¼ê° í–‰ë ¬)\n",
    "        # torch.tril: í•˜ì‚¼ê° í–‰ë ¬ ìƒì„± (ëŒ€ê°ì„  í¬í•¨)\n",
    "        # [[1, 0, 0],\n",
    "        #  [1, 1, 0],\n",
    "        #  [1, 1, 1]]\n",
    "        # ì´ìœ : ië²ˆì§¸ ë‹¨ì–´ëŠ” 0~ië²ˆì§¸ ë‹¨ì–´ë§Œ ë³¼ ìˆ˜ ìˆìŒ (ë¯¸ë˜ ëª» ë´„)\n",
    "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len))).bool().to(tgt.device)\n",
    "        \n",
    "        # ë‘ ë§ˆìŠ¤í¬ë¥¼ AND ì—°ì‚°ìœ¼ë¡œ ê²°í•©\n",
    "        # ì´ìœ : paddingë„ ë¬´ì‹œí•˜ê³  ë¯¸ë˜ë„ ê°€ë ¤ì•¼ í•¨\n",
    "        tgt_mask = tgt_mask & tgt_sub_mask\n",
    "        \n",
    "        return src_mask, tgt_mask\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            src (Tensor): ì†ŒìŠ¤ ì‹œí€€ìŠ¤ (ë‹¨ì–´ ID), shape: (batch, src_len)\n",
    "            tgt (Tensor): íƒ€ê²Ÿ ì‹œí€€ìŠ¤ (ë‹¨ì–´ ID), shape: (batch, tgt_len)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: ì¶œë ¥ logits, shape: (batch, tgt_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # ë§ˆìŠ¤í¬ ìƒì„±\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        \n",
    "        # ===== Encoder ë¶€ë¶„ =====\n",
    "        # 1. Source Embedding\n",
    "        # src: (batch, src_len) -> (batch, src_len, d_model)\n",
    "        src_embedded = self.src_embedding(src)\n",
    "        \n",
    "        # 2. Embedding Scaling\n",
    "        # math.sqrt(self.d_model)ì„ ê³±í•¨\n",
    "        # ì´ìœ : ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ë°©ë²•, positional encodingê³¼ì˜ ê· í˜• ìœ ì§€\n",
    "        #       ì„ë² ë”© ê°’ì˜ ìŠ¤ì¼€ì¼ì„ í‚¤ì›Œ ìœ„ì¹˜ ì •ë³´ì™€ ì„ì¼ ë•Œ ê· í˜• ë§ì¶¤\n",
    "        src_embedded = src_embedded * math.sqrt(self.d_model)\n",
    "        \n",
    "        # 3. Positional Encoding ì¶”ê°€\n",
    "        src_embedded = self.pos_encoding(src_embedded)\n",
    "        \n",
    "        # 4. Dropout ì ìš©\n",
    "        src_embedded = self.dropout(src_embedded)\n",
    "        \n",
    "        # 5. Encoder layers í†µê³¼\n",
    "        # ê° ë ˆì´ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í†µê³¼í•˜ë©° ì ì  ë” ì¶”ìƒì ì¸ í‘œí˜„ í•™ìŠµ\n",
    "        enc_output = src_embedded\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_output = layer(enc_output, src_mask)\n",
    "        \n",
    "        # ===== Decoder ë¶€ë¶„ =====\n",
    "        # 1. Target Embedding + Scaling\n",
    "        tgt_embedded = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # 2. Positional Encoding ì¶”ê°€\n",
    "        tgt_embedded = self.pos_encoding(tgt_embedded)\n",
    "        \n",
    "        # 3. Dropout ì ìš©\n",
    "        tgt_embedded = self.dropout(tgt_embedded)\n",
    "        \n",
    "        # 4. Decoder layers í†µê³¼\n",
    "        # ê° ë ˆì´ì–´ì—ì„œ encoder_outputì„ ì°¸ì¡°í•˜ì—¬ ì¶œë ¥ ìƒì„±\n",
    "        dec_output = tgt_embedded\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # ===== ìµœì¢… ì¶œë ¥ =====\n",
    "        # Linear layerë¡œ vocab í¬ê¸°ì˜ logits ìƒì„±\n",
    "        # (batch, tgt_len, d_model) -> (batch, tgt_len, tgt_vocab_size)\n",
    "        # ì´ìœ : ê° ìœ„ì¹˜ì—ì„œ ì–´ë–¤ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í• ì§€ ì ìˆ˜ ê³„ì‚°\n",
    "        output = self.fc_out(dec_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "print(\"=\"*60)\n",
    "print(\"ì™„ì „í•œ Transformer ëª¨ë¸ ìƒì„±\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "# ì‹¤ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ë§ê²Œ ì¡°ì • í•„ìš”\n",
    "src_vocab_size = 1000   # ì†ŒìŠ¤ ì–¸ì–´ ì–´íœ˜ í¬ê¸°\n",
    "tgt_vocab_size = 1000   # íƒ€ê²Ÿ ì–¸ì–´ ì–´íœ˜ í¬ê¸°\n",
    "d_model = 256           # ì‘ê²Œ ì„¤ì • (í•™ìŠµ ì†ë„ë¥¼ ìœ„í•´, ì‹¤ì œë¡œëŠ” 512)\n",
    "num_heads = 8           # ì–´í…ì…˜ í—¤ë“œ ê°œìˆ˜\n",
    "num_layers = 3          # ì¸ì½”ë”/ë””ì½”ë” ë ˆì´ì–´ ìˆ˜ (ì‹¤ì œë¡œëŠ” 6)\n",
    "d_ff = 512              # FFN hidden ì°¨ì› (ì‹¤ì œë¡œëŠ” 2048)\n",
    "\n",
    "# Transformer ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, \n",
    "                   num_heads, num_layers, d_ff)\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°\n",
    "# ì´ìœ : ëª¨ë¸ í¬ê¸° í™•ì¸, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nâœ“ ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"- ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n",
    "print(f\"- í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}\")\n",
    "print(f\"- ëª¨ë¸ í¬ê¸°: ì•½ {total_params * 4 / (1024**2):.2f} MB (float32 ê¸°ì¤€)\\n\")\n",
    "# float32: ê° íŒŒë¼ë¯¸í„°ê°€ 4ë°”ì´íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step7"
   },
   "source": [
    "---\n",
    "\n",
    "## 7ï¸âƒ£ ë‹¨ê³„ 7: ëª¨ë¸ í•™ìŠµ ì˜ˆì œ\n",
    "\n",
    "ì´ì œ ì‹¤ì œë¡œ Transformer ëª¨ë¸ì„ **í•™ìŠµ**í•´ë´…ì‹œë‹¤!\n",
    "\n",
    "### í•™ìŠµ ê³¼ì •\n",
    "1. ë°ì´í„° ì¤€ë¹„ (ë”ë¯¸ ë°ì´í„°)\n",
    "2. Loss í•¨ìˆ˜ & Optimizer ì„¤ì •\n",
    "3. Training Loop\n",
    "4. ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training"
   },
   "outputs": [],
   "source": [
    "def train_transformer():\n",
    "    \"\"\"\n",
    "    Transformer ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜\n",
    "    \n",
    "    ì—­í• : ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ë²ˆì—­(ë˜ëŠ” ë‹¤ë¥¸ seq2seq íƒœìŠ¤í¬) ìˆ˜í–‰ ëŠ¥ë ¥ íšë“\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Transformer ëª¨ë¸ í•™ìŠµ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ë””ë°”ì´ìŠ¤ ì„¤ì • (GPUê°€ ìˆìœ¼ë©´ GPU, ì—†ìœ¼ë©´ CPU ì‚¬ìš©)\n",
    "    # ì´ìœ : GPUë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ì†ë„ê°€ 10~100ë°° ë¹ ë¦„\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "    \n",
    "    # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "    # ì´ìœ : ëª¨ë¸ê³¼ ë°ì´í„°ê°€ ê°™ì€ ë””ë°”ì´ìŠ¤ì— ìˆì–´ì•¼ ì—°ì‚° ê°€ëŠ¥\n",
    "    model.to(device)\n",
    "    \n",
    "    # ì†ì‹¤ í•¨ìˆ˜ (Loss Function)\n",
    "    # CrossEntropyLoss: ë¶„ë¥˜ ë¬¸ì œì— ì‚¬ìš©ë˜ëŠ” ì†ì‹¤ í•¨ìˆ˜\n",
    "    # ì´ìœ : ê° ìœ„ì¹˜ì—ì„œ vocab_sizeê°œ ë‹¨ì–´ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ëŠ” ë¶„ë¥˜ ë¬¸ì œ\n",
    "    # ignore_index=0: padding í† í°(ID=0)ì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ì œì™¸\n",
    "    # ì´ìœ : paddingì€ ì‹¤ì œ ë‹¨ì–´ê°€ ì•„ë‹ˆë¯€ë¡œ ì˜ˆì¸¡í•  í•„ìš” ì—†ìŒ\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    # ì˜µí‹°ë§ˆì´ì € (Optimizer)\n",
    "    # Adam: Adaptive Moment Estimation, ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì˜µí‹°ë§ˆì´ì €\n",
    "    # ì´ìœ : 1) í•™ìŠµë¥ ì„ ìë™ìœ¼ë¡œ ì¡°ì • 2) ë¹ ë¥¸ ìˆ˜ë ´ 3) êµ¬í˜„ ê°„ë‹¨\n",
    "    # lr=0.0001: í•™ìŠµë¥  (learning rate)\n",
    "    # betas=(0.9, 0.98): Adamì˜ momentum íŒŒë¼ë¯¸í„° (ë…¼ë¬¸ ì„¤ì •)\n",
    "    # eps=1e-9: ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ ì‘ì€ ê°’\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, \n",
    "                          betas=(0.9, 0.98), eps=1e-9)\n",
    "    \n",
    "    # í•™ìŠµ ë°ì´í„° ì„¤ì •\n",
    "    # ì‹¤ì œë¡œëŠ” í† í°í™”ëœ ì‹¤ì œ ë¬¸ì¥ì„ ì‚¬ìš©í•´ì•¼ í•¨\n",
    "    batch_size = 32      # í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì¥ ìŒì˜ ê°œìˆ˜\n",
    "    src_seq_len = 10     # ì†ŒìŠ¤ ë¬¸ì¥ ê¸¸ì´\n",
    "    tgt_seq_len = 10     # íƒ€ê²Ÿ ë¬¸ì¥ ê¸¸ì´\n",
    "    \n",
    "    print(f\"\\ní•™ìŠµ ì„¤ì •:\")\n",
    "    print(f\"- Batch size: {batch_size}\")\n",
    "    print(f\"- Source sequence length: {src_seq_len}\")\n",
    "    print(f\"- Target sequence length: {tgt_seq_len}\")\n",
    "    print(f\"- Learning rate: 0.0001\")\n",
    "    \n",
    "    # í•™ìŠµ ë£¨í”„\n",
    "    num_epochs = 10  # ì „ì²´ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ë°˜ë³µí• ì§€\n",
    "    print(f\"\\ní•™ìŠµ ì‹œì‘ (Epochs: {num_epochs})...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # model.train(): í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    # ì´ìœ : Dropout, BatchNorm ë“±ì´ í•™ìŠµ ì‹œì—ë§Œ ì‘ë™\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ë”ë¯¸ ë°°ì¹˜ ìƒì„± (ì‹¤ì œë¡œëŠ” DataLoader ì‚¬ìš©)\n",
    "        # randint(1, 100, ...): 1~99 ì‚¬ì´ì˜ ëœë¤ ì •ìˆ˜ (0ì€ padding)\n",
    "        # ì´ìœ : ì‹¤ì œ í•™ìŠµ ë°ì´í„° ëŒ€ì‹  ëœë¤ ë°ì´í„°ë¡œ ë™ì‘ í™•ì¸\n",
    "        src = torch.randint(1, 100, (batch_size, src_seq_len)).to(device)\n",
    "        tgt = torch.randint(1, 100, (batch_size, tgt_seq_len)).to(device)\n",
    "        \n",
    "        # Gradient ì´ˆê¸°í™”\n",
    "        # ì´ìœ : ì´ì „ ë°°ì¹˜ì˜ gradientê°€ ëˆ„ì ë˜ì§€ ì•Šë„ë¡\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (Teacher Forcing)\n",
    "        # tgt[:, :-1]: ë§ˆì§€ë§‰ í† í° ì œì™¸ (ë””ì½”ë” ì…ë ¥)\n",
    "        # ì´ìœ : t ì‹œì ì˜ ì…ë ¥ìœ¼ë¡œ t+1 ì‹œì ì˜ ì¶œë ¥ ì˜ˆì¸¡\n",
    "        #       ì˜ˆ: \"<start> ë‚˜ëŠ” ë„ˆë¥¼\" -> \"ë‚˜ëŠ” ë„ˆë¥¼ ì‚¬ë‘í•´\"\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        \n",
    "        # ì†ì‹¤ ê³„ì‚°ì„ ìœ„í•œ reshape\n",
    "        # output: (batch, tgt_len-1, vocab_size) -> (batch*(tgt_len-1), vocab_size)\n",
    "        # ì´ìœ : CrossEntropyLossëŠ” 2D ì…ë ¥ í•„ìš”\n",
    "        output = output.reshape(-1, tgt_vocab_size)\n",
    "        \n",
    "        # target: tgt[:, 1:] (ì²« í† í° ì œì™¸)\n",
    "        # ì´ìœ : t ì‹œì ì˜ ì…ë ¥ì— ëŒ€í•œ ì •ë‹µì€ t+1 ì‹œì ì˜ í† í°\n",
    "        target = tgt[:, 1:].reshape(-1)\n",
    "        \n",
    "        # ì†ì‹¤ ê³„ì‚°\n",
    "        # ì´ìœ : ì˜ˆì¸¡ê³¼ ì •ë‹µì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ëª¨ë¸ ê°œì„  ë°©í–¥ íŒŒì•…\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass (ì—­ì „íŒŒ)\n",
    "        # ì´ìœ : ì†ì‹¤ì— ëŒ€í•œ ê° íŒŒë¼ë¯¸í„°ì˜ gradient ê³„ì‚°\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        # clip_grad_norm_: gradientì˜ normì„ ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ\n",
    "        # ì´ìœ : Gradient explosion ë°©ì§€ (gradientê°€ ë„ˆë¬´ ì»¤ì§€ë©´ í•™ìŠµ ë¶ˆì•ˆì •)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì € ìŠ¤í… (íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸)\n",
    "        # ì´ìœ : ê³„ì‚°ëœ gradientë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ê¸ˆì”© ìˆ˜ì •\n",
    "        #       Î¸_new = Î¸_old - lr * gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì¶œë ¥ (2 ì—í¬í¬ë§ˆë‹¤)\n",
    "        if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1:2d}/{num_epochs}] | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\nâœ“ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    print(\"\\nğŸ’¡ ì°¸ê³ : ì´ê²ƒì€ ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì˜ˆì œì…ë‹ˆë‹¤.\")\n",
    "    print(\"   ì‹¤ì œ ë²ˆì—­ íƒœìŠ¤í¬ë¥¼ ìœ„í•´ì„œëŠ”:\")\n",
    "    print(\"   1. ì‹¤ì œ ë²ˆì—­ ë°ì´í„°ì…‹ (ì˜ˆ: WMT, Multi30k)\")\n",
    "    print(\"   2. ì ì ˆí•œ í† í°í™” (Tokenization)\")\n",
    "    print(\"   3. ì¶©ë¶„í•œ í•™ìŠµ ì—í¬í¬ (ìˆ˜ì‹­~ìˆ˜ë°± ì—í¬í¬)\")\n",
    "    print(\"   4. Learning rate scheduling (í•™ìŠµë¥  ì¡°ì •)\")\n",
    "    print(\"   5. Validation setìœ¼ë¡œ í‰ê°€\")\n",
    "\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "train_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š í•µì‹¬ ê°œë… ì •ë¦¬\n",
    "\n",
    "### 1. Self-Attention\n",
    "- ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµ\n",
    "- Q, K, Vë¥¼ ì´ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "- **\"ì–´ë–¤ ë‹¨ì–´ê°€ ì–´ë–¤ ë‹¨ì–´ì™€ ê´€ë ¨ìˆëŠ”ê°€?\"**\n",
    "\n",
    "### 2. Multi-Head Attention\n",
    "- ì—¬ëŸ¬ ê´€ì ì—ì„œ ê´€ê³„ í•™ìŠµ\n",
    "- ê° headê°€ ë‹¤ë¥¸ íŒ¨í„´ì„ í•™ìŠµ\n",
    "- **\"ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ë¬¸ì¥ì„ ì´í•´\"**\n",
    "\n",
    "### 3. Positional Encoding\n",
    "- ë‹¨ì–´ì˜ ìˆœì„œ ì •ë³´ ì¶”ê°€\n",
    "- sin, cos í•¨ìˆ˜ ì´ìš©\n",
    "- **\"ì²« ë²ˆì§¸ ë‹¨ì–´ì¸ì§€, ë§ˆì§€ë§‰ ë‹¨ì–´ì¸ì§€\"**\n",
    "\n",
    "### 4. Encoder-Decoder êµ¬ì¡°\n",
    "- **Encoder**: ì…ë ¥ ë¬¸ì¥ì„ ì´í•´\n",
    "- **Decoder**: ì¶œë ¥ ë¬¸ì¥ì„ ìƒì„±\n",
    "- **Cross-Attention**: Decoderê°€ Encoder ì •ë³´ ì°¸ì¡°\n",
    "\n",
    "### 5. Residual Connection & Layer Normalization\n",
    "- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì˜ í•™ìŠµì„ ì•ˆì •í™”\n",
    "- Gradient vanishing ë¬¸ì œ ì™„í™”\n",
    "- **\"ì •ë³´ì˜ íë¦„ì„ ì›í™œí•˜ê²Œ\"**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ì‹¤ìŠµ ê³¼ì œ\n",
    "1. **Attention Weights ì‹œê°í™”**\n",
    "   - ì–´ë–¤ ë‹¨ì–´ê°€ ì–´ë–¤ ë‹¨ì–´ì— ì£¼ëª©í•˜ëŠ”ì§€ í™•ì¸\n",
    "   - Heatmapìœ¼ë¡œ í‘œí˜„\n",
    "\n",
    "2. **ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµ**\n",
    "   - Multi30k ë°ì´í„°ì…‹ (ì˜ì–´-ë…ì¼ì–´)\n",
    "   - ê°„ë‹¨í•œ ì˜ì–´-í•œêµ­ì–´ ë²ˆì—­\n",
    "\n",
    "3. **Beam Search êµ¬í˜„**\n",
    "   - Greedy decoding ê°œì„ \n",
    "   - ë” ì¢‹ì€ ë²ˆì—­ ê²°ê³¼\n",
    "\n",
    "### ì‹¬í™” í•™ìŠµ\n",
    "- BERT (Encoder only)\n",
    "- GPT (Decoder only)\n",
    "- Vision Transformer (ì´ë¯¸ì§€ì— ì ìš©)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "1. **ë…¼ë¬¸**: \"Attention is All You Need\" (Vaswani et al., 2017)\n",
    "2. **ë¸”ë¡œê·¸**: The Illustrated Transformer by Jay Alammar\n",
    "3. **ì½”ë“œ**: The Annotated Transformer\n",
    "4. **ê°•ì˜**: CS224N (Stanford NLP)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!\n",
    "\n",
    "Transformerì˜ ëª¨ë“  í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤!  \n",
    "ì´ì œ ì—¬ëŸ¬ë¶„ì€ í˜„ëŒ€ NLPì˜ ê¸°ë°˜ì„ ì´í•´í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ğŸš€\n",
    "\n",
    "**\"Attention is All You Need\" - ê·¸ë¦¬ê³  ë‹¹ì‹ ì˜ ë…¸ë ¥ë„!** ğŸ’ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises"
   },
   "source": [
    "---\n",
    "\n",
    "## ğŸ’» ì¶”ê°€ ì‹¤ìŠµ ë¬¸ì œ\n",
    "\n",
    "ì•„ë˜ ì…€ì—ì„œ ì§ì ‘ ì½”ë“œë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise1"
   },
   "outputs": [],
   "source": [
    "# ì‹¤ìŠµ 1: Attention weightsë¥¼ ì‹œê°í™”í•´ë³´ì„¸ìš”\n",
    "# íŒíŠ¸: matplotlibì˜ imshowë‚˜ seabornì˜ heatmap ì‚¬ìš©\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TODO: ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise2"
   },
   "outputs": [],
   "source": [
    "# ì‹¤ìŠµ 2: ê°„ë‹¨í•œ ì¶”ë¡ (inference) í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”\n",
    "# ì…ë ¥: \"I love you\"\n",
    "# ì¶œë ¥: \"ë‚˜ëŠ” ë„ˆë¥¼ ì‚¬ë‘í•´\" (í† í° IDë¡œ)\n",
    "\n",
    "def translate(model, src_sentence, max_len=20):\n",
    "    \"\"\"\n",
    "    ë¬¸ì¥ì„ ë²ˆì—­í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        model: í•™ìŠµëœ Transformer ëª¨ë¸\n",
    "        src_sentence: ì…ë ¥ ë¬¸ì¥ (í† í° ID ë¦¬ìŠ¤íŠ¸)\n",
    "        max_len: ìµœëŒ€ ìƒì„± ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        ë²ˆì—­ ê²°ê³¼ (í† í° ID ë¦¬ìŠ¤íŠ¸)\n",
    "    \"\"\"\n",
    "    # TODO: ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
    "    # íŒíŠ¸: 1) model.eval() ëª¨ë“œë¡œ ì „í™˜\n",
    "    #       2) torch.no_grad() ì‚¬ìš©\n",
    "    #       3) <start> í† í°ë¶€í„° ì‹œì‘í•´ì„œ í•œ ë‹¨ì–´ì”© ìƒì„±\n",
    "    #       4) <end> í† í°ì´ ë‚˜ì˜¤ê±°ë‚˜ max_lenì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise3"
   },
   "outputs": [],
   "source": [
    "# ì‹¤ìŠµ 3: Positional Encodingì˜ íŒ¨í„´ì„ ì‹œê°í™”í•´ë³´ì„¸ìš”\n",
    "# íŒíŠ¸: plt.imshowë¥¼ ì‚¬ìš©í•˜ì—¬ (max_len, d_model) í–‰ë ¬ ì‹œê°í™”\n",
    "\n",
    "# TODO: ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
